# Written by Mr. Inal Mashukov
# this is a sample set of hyperparameters, adjust and fine tune these as you see fit.
# python transformer_mul2018_train-with_isTest-different_decoder.py -gpu 3 -t m -d PEMS -su True -n PEMS_10_30
[params]
head_size = 128
num_heads = 8
ff_dim = 256
num_transformer_blocks = 4
mlp_units = 256
dropout = 0.33
mlp_dropout = 0.33
initial_lr = 0.00001
batch_size = 32
epoch = 400
earlystop_patience = 50
earlystop_min_delta = 0.0000001
l2_reg = 0.01
encoder_loop = 16
decoder_loop = 16
resolution = [50,50,50,50]
n_encoder = 2         
n_decoder = 2

# for kdd
d_model = 128
nhead = 32
feat_dim = 200
max_len = 128
num_layers = 16
dim_feedforward = 256
